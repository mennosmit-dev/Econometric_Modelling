**For full details feel free to download the pdf in the map.**

# ğŸ“Š Income Level Prediction with Random Forests ğŸŒ³

## ğŸ” Overview

This project implements a machine learning pipeline to predict income levels based on tabular data using Random Forest classifiers. It performs feature selection, hyperparameter tuning, model training, and final prediction generation with nested cross-validation to ensure robust evaluation.

The code was originally developed in a Jupyter notebook and later cleaned up for clarity and reproducibility.

---

## ğŸ“ Contents

- `Predicting_Income_Level.ipynb` / `.py`  
  Main script that:  
  - Loads data and a codebook describing features.  
  - Performs nested cross-validation for hyperparameter tuning and feature selection.  
  - Saves tuning results and visualizes the impact of hyperparameters.  
  - Trains a final Random Forest model on selected features.  
  - Generates predictions on the test dataset and saves them to a text file.

- `codebook.csv` ğŸ“–  
  CSV file describing features and their meanings.

- `train.csv` ğŸ‹ï¸â€â™‚ï¸  
  Training dataset with features and target variable.

- `test.csv` ğŸ§ª  
  Test dataset with features only.

- `oos_accuracies.csv` ğŸ“Š  
  Output file containing cross-validation results for hyperparameter tuning and feature selection.

- `mean_accuracy_hyperparameters.png` ğŸ“ˆ  
  Visualization of mean accuracy by number of features and hyperparameter combinations.

- `features_prediction.png` ğŸŒŸ  
  Bar plot of feature importances used in the final model.

- `predictions.txt` ğŸ“  
  Final predictions on the test set generated by the model.

---

## âš™ï¸ Requirements

- Python 3.7 or later  
- Packages:  
  - numpy â•  
  - pandas ğŸ¼  
  - matplotlib ğŸ¨  
  - scikit-learn ğŸ¤–  
  - tqdm â³  

Install the packages using pip if needed:

HAI!bash
pip install numpy pandas matplotlib scikit-learn tqdm
HAI!

---

## ğŸš€ How to Run

1. Place the dataset files (`train.csv`, `test.csv`, and `codebook.csv`) in the working directory.

2. Run the main script (`Predicting_Income_Level.ipynb` or `.py`) to:

   - Load and prepare data.  
   - Perform nested cross-validation to tune hyperparameters and select important features.  
   - Save tuning results and generate visualizations.  
   - Train the final model and produce predictions for the test dataset.  
   - Save predictions to `predictions.txt`.

3. View output plots (`mean_accuracy_hyperparameters.png`, `features_prediction.png`) for insights on hyperparameter tuning and feature importance.

---

## ğŸ“‚ Output Files

- `oos_accuracies.csv`: Cross-validation results during tuning.  
- `mean_accuracy_hyperparameters.png`: Accuracy visualization for different hyperparameter combinations and feature counts.  
- `features_prediction.png`: Feature importance bar plot.  
- `predictions.txt`: Final predictions on test data.

---

## ğŸ’¡ Notes

- Random Forest models do not require feature standardization or normalization.  
- Feature selection is based on feature importance with a threshold to reduce dimensionality.  
- Nested cross-validation ensures unbiased performance estimation and hyperparameter selection.  
- Hyperparameters tuned include number of trees, max depth, min samples per split/leaf, and max features.

---

Feel free to reach out if you have questions or want to contribute! ğŸ™Œ
