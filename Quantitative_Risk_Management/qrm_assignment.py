# -*- coding: utf-8 -*-
"""QRM Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rYEjCIi6w9QTF3luLg_9wNbuQMPX-0sT

QUANTITATIVE RISK MANAGEMENT
"""

!pip install arch
!pip install copulas

pip install --upgrade copulas

#Imports
import pandas as pd
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt
from scipy.stats import chi2
from scipy.stats import norm

from arch import arch_model
from scipy.stats import t
import copulas
from copulas.bivariate import Clayton

#Opening the data, this file needs to be the same for all analysis!!!
df = pd.read_excel(r"C:\Users\Menno Smit\Downloads\Data_QRM.xlsx", sheet_name="Data", skiprows=1, usecols=[0, 1])

# Check the number of columns in the DataFrame (optional)
num_columns = len(df.columns)
print(f"The DataFrame has {num_columns} columns.")

# Select the first two columns (if there are at least two columns)
if num_columns >= 2:
  df = df.iloc[:, :2]  # Select the first two columns using slicing
  df.columns = ['Stock 1', 'Stock 2']  # Assign column names
else:
  print("The DataFrame has less than two columns. Skipping column name assignment.")

stock1 = df.iloc[:, 0].to_numpy()
stock2 = df.iloc[:, 1].to_numpy()



print(stock1)
print(stock2)

"""METHOD 1: VARIANCE-COVARIANCE APPROACH"""

#dataFrame
LossReturns = np.array(df)

NumObs = LossReturns.shape[0] #amount of observations

covmatrix = np.cov(LossReturns, rowvar=False)  #False ensures colomns variables

print("Mean Stock 1", format(np.mean(LossReturns[:, 0]), ".4g"))
print("Var Stock 1", format(covmatrix[0,0], ".4g"))
print("Mean Stock 2", format(np.mean(LossReturns[:, 1]), ".4g"))
print("Var Stock 2", format(covmatrix[1,1], ".4g"))
print("Covariance", format(covmatrix[0,1], ".4g"))

meanPF = np.mean(0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1])
varPF = np.dot(np.dot(np.array([0.4, 0.6]), covmatrix), np.array([0.4, 0.6]))
print("Mean Portfolio", format(meanPF, ".4g"))
print("Var Portfolio", format(varPF, ".4g"))
print("99% Quantile:", format(norm.ppf(0.99, meanPF, np.sqrt(varPF)),".4g"))

#Deriving first and second sample moment

data = np.vstack((stock1, stock2)).T
weights = np.array([0.4, 0.6])

portfolio_returns = np.dot(data, weights)
centered_portfolio_returns = portfolio_returns - np.mean(portfolio_returns)

mean_vector = np.mean(data, axis = 0)
centered_data = data - mean_vector

cov_matrix = (1 / (len(stock1) - 1)) * np.dot(centered_data.T, centered_data)

mu1 = sum(stock1)/len(stock1)
mu2 = sum(stock2)/len(stock2)

#VaR en ES based on sample moments (variance-covariance) - oneday 99% var

#I searched that it is common to first construct portfolio returns before handling the VaR, slightly different results. I believe the ones with a 1 behind them are correct.

portfolio_mean = np.dot(weights, mean_vector)
portfolio_mean1 = np.mean(portfolio_returns)
portfolio_var = np.dot(np.dot(weights.T, cov_matrix), weights)
portfolio_var1 = (1/ (len(portfolio_returns - 1))) * np.dot(centered_portfolio_returns.T, centered_portfolio_returns)
portfolio_sd = np.sqrt(portfolio_var)
portfolio_sd1 = np.sqrt(portfolio_var1)

z_score = norm.ppf(0.01)

# time -1 because those are in losses
VaR_99_varcov = -1 * (portfolio_mean + z_score * portfolio_sd)

VaR_99_varcov1 = -1 * (portfolio_mean + z_score * portfolio_sd1)

#answer is in percentages, so need to multiply this with wealth (which is 100 million in assignment)
print(VaR_99_varcov, VaR_99_varcov1)

"""METHOD 2: HISTORICAL SIMULATION"""

PFreturn = 0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]
print("99% Quantile:", format(np.quantile(PFreturn, 0.99), ".4g"))

#One comment here. Here we make a joint EDF of the historical distributions. Maybe it is better to first construct a portfolio loss array and then use that EDF.
#Internet also says that this is wrong.
#Order the returns of the seperate returns

stock1_ordered = stock1[np.argsort(stock1)]
stock2_ordered = stock2[np.argsort(stock2)]

#Constructing function
edf_values = np.zeros((len(stock1), len(stock2)))

for i in range(len(stock1)):
    for j in range(len(stock2)):
        count = np.sum((stock1 <= stock1[i]) & (stock2 <= stock2[j]))
        edf_values[i, j] = count / (len(stock1) * len(stock2))


closest_indices = np.unravel_index(np.argmin(np.abs(edf_values - 0.01)), edf_values.shape)
var_99_stock1 = stock1[closest_indices[0]]
var_99_stock2 = stock2[closest_indices[1]]

VaR99_HS = weights[0] * var_99_stock1 + weights[1] * var_99_stock2

print(VaR99_HS)

#Checking if results differ with EDF of the return.

portfolio_returns = np.dot(weights, data.T)

portfolio_ordered = portfolio_returns[(np.argsort(portfolio_returns))]

edf_values_portfolio = np.zeros(len(portfolio_returns))

for i in range(len(portfolio_returns)):
    count = np.sum(portfolio_returns <= portfolio_returns[i])
    edf_values_portfolio[i] = count / len(portfolio_returns)


closest_index = np.argmin(np.abs(edf_values_portfolio - 0.01)) # I think that you need to round up here, but not 100% sure
VaR_99_portf_HS = -1 * portfolio_returns[closest_index]


print(VaR_99_portf_HS)

#Okay, these values differ a lot (see above). I guess that this one is better since you have less VaR due to diversification effects.

"""METHOD 3: NORMAL MIXTURE MODEL"""

#First constructing a Jarque-Bera test on portfolio returns
n = len(portfolio_returns)


mu = np.mean(portfolio_returns)
sd = np.sqrt(1/n * sum(centered_portfolio_returns**2))
b = 1/ n * sum((centered_portfolio_returns/sd)**3) #skewness
k = 1/n * sum((centered_portfolio_returns/sd)**4)  #kurtosis

T = n / 6 * (b**2 + (1 / 4) * (k - 3)**2)

print(mu, sd, b, k, T)
#Slightly right skewed and more concentrated around the middle than NORMAL

p_value = 1 - chi2.cdf(T, df=2)
print(p_value)
#p-value is 0, so not Normal

print(portfolio_returns[599])

import numpy as np
from scipy.stats import norm, jarque_bera

# Define the Mixture
Mixture = 0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]

# Jarque-Bera test
jb_stat, jb_pvalue = jarque_bera(Mixture)
print(f"Jarque-Bera test statistic: {jb_stat}, p-value: {jb_pvalue}")

# Initialize variables
p = np.zeros((len(Mixture), 2))  # Adapt to dataset size dynamically
L = [0, 1e-5]  # Ensure L has at least two values
j = 1
X = 1
max_iterations = 1000  # Add an iteration limit

# Initialize parameters
theta = np.array([
    0.3,
    np.mean(LossReturns[:, 0]), max(np.std(LossReturns[:, 0]), 1e-5),  # Ensure std is not zero
    np.mean(LossReturns[:, 1]), max(np.std(LossReturns[:, 1]), 1e-5)
])

# EM Algorithm
while abs(X) > 1e-6 and j < max_iterations:
    j += 1
    Q = 0
    X = 0

    # E-step
    for i in range(len(Mixture)):
        denom = (
            norm.pdf(Mixture[i], theta[1], theta[2]) * theta[0] +
            norm.pdf(Mixture[i], theta[3], theta[4]) * (1 - theta[0])
        )
        if denom == 0:
            denom = 1e-10  # Avoid division by zero

        p[i, 0] = (norm.pdf(Mixture[i], theta[1], theta[2]) * theta[0]) / denom
        p[i, 1] = 1 - p[i, 0]

    # Q function
    for i in range(len(Mixture)):
        if p[i, 0] > 0 and p[i, 1] > 0:  # Avoid log(0)
            Q += p[i, 0] * (np.log(theta[0]) + np.log(norm.pdf(Mixture[i], theta[1], theta[2]))) + \
                 p[i, 1] * (np.log(1 - theta[0]) + np.log(norm.pdf(Mixture[i], theta[3], theta[4])))

    L.append(Q)
    if len(L) > 2:
        L.pop(0)  # Keep only the last two values

    # M-step
    sum_p0 = np.sum(p[:, 0])
    sum_p1 = np.sum(p[:, 1])

    theta[0] = sum_p0 / len(Mixture)

    if sum_p0 > 0:
        theta[1] = np.dot(Mixture, p[:, 0]) / sum_p0
        theta[2] = max(np.sqrt(np.dot((Mixture - theta[1])**2, p[:, 0]) / sum_p0), 1e-5)  # Ensure non-zero std

    if sum_p1 > 0:
        theta[3] = np.dot(Mixture, p[:, 1]) / sum_p1
        theta[4] = max(np.sqrt(np.dot((Mixture - theta[3])**2, p[:, 1]) / sum_p1), 1e-5)  # Ensure non-zero std

    X = L[-1] - L[-2]

    # Stop if NaN values appear
    if np.isnan(theta).any():
        print("NaN detected in theta. Stopping EM Algorithm.")
        break

print(f"Converged in {j} iterations")
print("Theta values:", theta)

# Monte Carlo Simulation
lambda_ = theta[0]
Tot = 0
num_MC_sims = 10
num_MC_samples = 1000

for _ in range(num_MC_sims):
    MC = np.zeros(num_MC_samples)
    bern = np.random.binomial(1, lambda_, num_MC_samples)

    for i in range(num_MC_samples):
        if bern[i] == 1:
            MC[i] = theta[1] + theta[2] * np.random.normal(0, 1)
        else:
            MC[i] = theta[3] + theta[4] * np.random.normal(0, 1)

    Tot += np.quantile(MC, 0.99)

VaRQ3 = Tot / num_MC_sims
print("VaR at 99% confidence level:", VaRQ3)

#EM algorithm for estimating the parameters for the two Normal distributions

#initialize the parameters
mean = np.mean(portfolio_returns)
sd = np.std(portfolio_returns)
print(mean, sd)

lamda, mu1, mu2, sigma1, sigma2 = mean - 0.1 , mean +0.1, 4, 5, 0.5
tol = 1e-6
L_t_min_1 = 1
L_t = 0
iteration = 0
iteration_limit = 10

n = len(portfolio_returns)



while abs(L_t_min_1 - L_t) > tol or iteration < iteration_limit:
    Q = []
    for i in range(n):
        #E-step
        p1 =  (norm.pdf(portfolio_returns[i], mu1, sigma1) * lamda ) / (norm.pdf(portfolio_returns[i], mu1, sigma1) * lamda + norm.pdf(portfolio_returns[i], mu2, sigma2) * (1 - lamda))
        p2 = 1 - p1

    #M-step
    lamda = np.mean(p1)

    mu1 = np.sum((portfolio_returns - mu1)**2) / np.sum(p1)
    mu2 = np.sum((portfolio_returns - mu2)**2) / np.sum(p2)

    sigma1 = np.sqrt(np.sum(p1 * (portfolio_returns - mu1)**2) / np.sum(p1))
    sigma2 = np.sqrt(np.sum(p2 * (portfolio_returns - mu2)**2) / np.sum(p2))

    L_t_min_1 = L_t
    L_t = np.sum(np.log(lamda * norm.pdf(portfolio_returns, mu1, sigma1) + (1 - lamda) * norm.pdf(portfolio_returns, mu2, sigma2)))
    iteration += 1

    print(lamda, mu1, mu2, sigma1, sigma2)

import numpy as np
from scipy.stats import norm

def EM_algorithm(portfolio_returns, K=2, iteration_limit=100, tol=1e-6):
    """
    Performs the EM algorithm for a normal mixture model with two components.

    Args:
        portfolio_returns (np.ndarray): Array of portfolio returns.
        K (int, optional): Number of mixture components. Defaults to 2.
        iteration_limit (int, optional): Maximum number of iterations. Defaults to 100.
        tol (float, optional): Tolerance for convergence. Defaults to 1e-6.

    Returns:
        tuple: Estimated parameters (lambda0, mu1, sigma1, mu2, sigma2).
    """

    n = len(portfolio_returns)

    # Calculate initial parameters based on data
    mean = np.mean(portfolio_returns)
    std_dev = np.std(portfolio_returns)

    # Initialize parameters
    lamda = 0.5
    mu1 = mean - 0.3 * std_dev  # Slightly wider initial separation
    mu2 = mean + 0.3 * std_dev
    sigma1 = std_dev / 2
    sigma2 = std_dev * 2

    L_t_min_1 = 1
    L_t = 0
    iteration = 0

    while abs(L_t_min_1 - L_t) > tol and iteration < iteration_limit:
        L_t = L_t_min_1

        # E-step
        p1 = (lamda * norm.pdf(portfolio_returns, mu1, sigma1)) / (lamda * norm.pdf(portfolio_returns, mu1, sigma1) + (1 - lamda) * norm.pdf(portfolio_returns, mu2, sigma2))
        p2 = 1 - p1

        # M-step (Handle potential division by zero)
        lamda = np.mean(p1)

        mu1 = np.sum(p1 * portfolio_returns) / (np.sum(p1) + 1e-8)
        mu2 = np.sum(p2 * portfolio_returns) / (np.sum(p2) + 1e-8)

        sigma1 = np.sqrt(np.sum(p1 * (portfolio_returns - mu1)**2) / (np.sum(p1) + 1e-8))
        sigma2 = np.sqrt(np.sum(p2 * (portfolio_returns - mu2)**2) / (np.sum(p2) + 1e-8))

        # Calculate L_t with safeguards against log(0)
        log_term1 = p1 * (np.log(lamda) + np.log(norm.pdf(portfolio_returns, mu1, sigma1)))
        log_term2 = p2 * (np.log(1 - lamda) + np.log(norm.pdf(portfolio_returns, mu2, sigma2)))

        # Replace log(0) with a very small negative value (e.g., -100)
        log_term1[log_term1 == -np.inf] = -100
        log_term2[log_term2 == -np.inf] = -100

        L_t = np.sum(log_term1 + log_term2)

        iteration += 1

    return lamda, mu1, sigma1, mu2, sigma2

# Example usage
parameters = EM_algorithm(portfolio_returns)
print(parameters)


#Now apply the monte carlo method in order to estimate the VaR


def simulate_mixture_observations(m, lambda_, mu1, mu2, sigma1, sigma2):
    """
    Simulates observations from a two-component Gaussian mixture model.

    Args:
        m (int): Number of observations to simulate.
        lambda_ (float): Probability of belonging to the first component.
        mu1 (float): Mean of the first component.
        mu2 (float): Mean of the second component.
        sigma1 (float): Standard deviation of the first component.
        sigma2 (float): Standard deviation of the second component.

    Returns:
        np.ndarray: Array of simulated observations.
    """

    B = np.random.binomial(1, lamda, size=m)  # Simulate Bernoulli variables
    Z = np.random.standard_normal(size=m)  # Simulate standard normal variables

    X = np.where(B == 1, mu1 + sigma1 * Z, mu2 + sigma2 * Z)  # Combine components

    return X

# Example usage
m = len(portfolio_returns)  # Number of observations
lamda = parameters[0]
mu1 = parameters[1]
mu2 = parameters[3]
sigma1 = parameters[2]
sigma2 = parameters[4]


observations = simulate_mixture_observations(m, lamda, mu1, mu2, sigma1, sigma2)
print(observations)

#Calculating VaR of the simulated observations of 99%
VaR_99_monte_carlo = -np.percentile(observations, 1)
print(VaR_99_monte_carlo)

#First constructing a Jarque-Bera test on portfolio returns
n = len(portfolio_returns)


mu = np.mean(portfolio_returns)
sd = np.sqrt(1/n * sum(centered_portfolio_returns**2))
b = 1/ n * sum((centered_portfolio_returns/sd)**3) #skewness
k = 1/n * sum((centered_portfolio_returns/sd)**4)  #kurtosis

T = n / 6 * (b*2 + (1 / 4) * (k - 3)*2)

print(mu, sd, b, k, T)
#Slightly right skewed and more concentrated around the middle than NORMAL

p_value = 1 - chi2.cdf(T, df=2)
print(p_value)
#p-value is 0, so not Normal

print(portfolio_returns[599])



def EM_algorithm(portfolio_returns, StockLoss, K=2, iteration_limit=100, tol=1e-6):
    """
    Performs the EM algorithm for a two-component normal mixture model.

    Args:
        portfolio_returns (np.ndarray): Array of portfolio returns (the mixture).
        StockLoss (np.ndarray): Original 2000x2 data array.
        K (int, optional): Number of mixture components. Defaults to 2.
        iteration_limit (int, optional): Maximum number of iterations. Defaults to 100.
        tol (float, optional): Tolerance for convergence. Defaults to 1e-6.

    Returns:
        tuple: Estimated parameters (lambda0, mu1, sigma1, mu2, sigma2).
    """
    n = len(portfolio_returns)

    # Initialize parameters from StockLoss (to match R)
    lamda = 0.3
    mu1 = np.mean(LossReturns[:, 0])
    sigma1 = np.std(LossReturns[:, 0], ddof=1)
    mu2 = np.mean(LossReturns[:, 1])
    sigma2 = np.std(LossReturns[:, 1], ddof=1)

    # Initialize log-likelihood values for convergence checking.
    L_prev = -np.inf
    L_curr = 0.0
    iteration = 0

    # EM iterations: continue until the change in log-likelihood is below tol or iteration limit is reached.
    while abs(L_curr - L_prev) > tol and iteration < iteration_limit:
        L_prev = L_curr

        # E-step: compute responsibilities.
        p1 = (lamda * norm.pdf(portfolio_returns, mu1, sigma1)) / (
             lamda * norm.pdf(portfolio_returns, mu1, sigma1) +
             (1 - lamda) * norm.pdf(portfolio_returns, mu2, sigma2))
        p2 = 1 - p1

        # M-step: update parameters.
        lamda = np.mean(p1)
        mu1 = np.sum(p1 * portfolio_returns) / (np.sum(p1) + 1e-8)
        mu2 = np.sum(p2 * portfolio_returns) / (np.sum(p2) + 1e-8)
        sigma1 = np.sqrt(np.sum(p1 * (portfolio_returns - mu1)**2) / (np.sum(p1) + 1e-8))
        sigma2 = np.sqrt(np.sum(p2 * (portfolio_returns - mu2)**2) / (np.sum(p2) + 1e-8))

        # Compute the expected log-likelihood with safeguards against log(0).
        log_term1 = p1 * (np.log(lamda + 1e-12) + np.log(norm.pdf(portfolio_returns, mu1, sigma1) + 1e-12))
        log_term2 = p2 * (np.log(1 - lamda + 1e-12) + np.log(norm.pdf(portfolio_returns, mu2, sigma2) + 1e-12))
        log_term1[log_term1 == -np.inf] = -100
        log_term2[log_term2 == -np.inf] = -100

        L_curr = np.sum(log_term1 + log_term2)
        iteration += 1

    return lamda, mu1, sigma1, mu2, sigma2

# -----------------------
# Example usage:
# Assume StockLoss is your 2000x2 NumPy array.
# In R, you set: Mixture <- 0.4 * StockLoss[,1] + 0.6 * StockLoss[,2]
portfolio_returns = 0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]

parameters = EM_algorithm(portfolio_returns, LossReturns)
print("Estimated Parameters (EM):", parameters)


def simulate_mixture_observations(m, lambda_, mu1, mu2, sigma1, sigma2):
    """
    Simulates observations from a two-component Gaussian mixture model.

    Args:
        m (int): Number of observations to simulate.
        lambda_ (float): Probability of belonging to the first component.
        mu1 (float): Mean of the first component.
        mu2 (float): Mean of the second component.
        sigma1 (float): Standard deviation of the first component.
        sigma2 (float): Standard deviation of the second component.

    Returns:
        np.ndarray: Array of simulated observations.
    """
    B = np.random.binomial(1, lambda_, size=m)
    Z = np.random.standard_normal(size=m)
    X = np.where(B == 1, mu1 + sigma1 * Z, mu2 + sigma2 * Z)
    return X

# -----------------------
# Monte Carlo Simulation for VaR Estimation
num_sim = 10
sample_size = 100000
Tot = 0.0

lamda = parameters[0]
mu1 = parameters[1]
mu2 = parameters[3]
sigma1 = parameters[2]
sigma2 = parameters[4]

for t in range(num_sim):
    MC = simulate_mixture_observations(sample_size, lamda, mu1, mu2, sigma1, sigma2)
    Tot += np.percentile(MC, 99)

VaR_99_monte_carlo = Tot / num_sim
print("Estimated 99% VaR from Monte Carlo simulation:", VaR_99_monte_carlo)

import numpy as np
from scipy.stats import norm

def EM_algorithm(portfolio_returns, LossReturns, iteration_limit=100, tol=1e-6):
    """
    Performs the EM algorithm for a two-component normal mixture model.
    This version mimics the R code by initializing parameters using the separate columns of LossReturns.

    Args:
        portfolio_returns (np.ndarray): The mixture returns (computed as 0.4*LossReturns[:,0] + 0.6*LossReturns[:,1]).
        LossReturns (np.ndarray): The original 2000x2 data array.
        iteration_limit (int, optional): Maximum number of iterations.
        tol (float, optional): Convergence tolerance.

    Returns:
        tuple: Estimated parameters (lambda, mu1, sigma1, mu2, sigma2)
               where
                 lambda  = mixing weight for component 1,
                 mu1     = weighted mean for component 1,
                 sigma1  = weighted standard deviation for component 1,
                 mu2     = weighted mean for component 2,
                 sigma2  = weighted standard deviation for component 2.
    """
    n = len(portfolio_returns)

    # Initialize parameters exactly as in R:
    lamda = 0.3
    mu1 = np.mean(LossReturns[:, 0])
    sigma1 = np.std(LossReturns[:, 0], ddof=1)
    mu2 = np.mean(LossReturns[:, 1])
    sigma2 = np.std(LossReturns[:, 1], ddof=1)

    # Compute initial log-likelihood using the starting parameters.
    L_prev = 0.0
    for i in range(n):
        d1 = norm.pdf(portfolio_returns[i], loc=mu1, scale=sigma1)
        d2 = norm.pdf(portfolio_returns[i], loc=mu2, scale=sigma2)
        L_prev += (lamda * (np.log(lamda + 1e-12) + np.log(d1 + 1e-12)) +
                   (1 - lamda) * (np.log(1 - lamda + 1e-12) + np.log(d2 + 1e-12)))
    L_curr = L_prev  # Initialize L_curr with the initial log-likelihood
    iteration = 0

    # EM iterations: repeat until the change in log-likelihood is below tol.
    while abs(L_curr - L_prev) > tol and iteration < iteration_limit:
        L_prev = L_curr

        # E-step: compute responsibilities for each observation.
        p1 = np.empty(n)
        for i in range(n):
            d1 = norm.pdf(portfolio_returns[i], loc=mu1, scale=sigma1)
            d2 = norm.pdf(portfolio_returns[i], loc=mu2, scale=sigma2)
            p1[i] = (lamda * d1) / (lamda * d1 + (1 - lamda) * d2 + 1e-12)
        p2 = 1 - p1

        # M-step: update parameters using weighted averages.
        lamda = np.mean(p1)
        mu1 = np.sum(p1 * portfolio_returns) / (np.sum(p1) + 1e-8)
        sigma1 = np.sqrt(np.sum(p1 * (portfolio_returns - mu1)**2) / (np.sum(p1) + 1e-8))
        mu2 = np.sum(p2 * portfolio_returns) / (np.sum(p2) + 1e-8)
        sigma2 = np.sqrt(np.sum(p2 * (portfolio_returns - mu2)**2) / (np.sum(p2) + 1e-8))

        # Compute the expected log-likelihood with safeguards against log(0).
        L_curr = 0.0
        for i in range(n):
            d1 = norm.pdf(portfolio_returns[i], loc=mu1, scale=sigma1)
            d2 = norm.pdf(portfolio_returns[i], loc=mu2, scale=sigma2)
            L_curr += (p1[i] * (np.log(lamda + 1e-12) + np.log(d1 + 1e-12)) +
                       p2[i] * (np.log(1 - lamda + 1e-12) + np.log(d2 + 1e-12)))
        iteration += 1

    return lamda, mu1, sigma1, mu2, sigma2


def simulate_mixture_observations(m, lambda_, mu1, mu2, sigma1, sigma2):
    """
    Simulates m observations from a two-component Gaussian mixture model.

    Args:
        m (int): Number of observations to simulate.
        lambda_ (float): Probability of belonging to the first component.
        mu1 (float): Mean of the first component.
        mu2 (float): Mean of the second component.
        sigma1 (float): Standard deviation of the first component.
        sigma2 (float): Standard deviation of the second component.

    Returns:
        np.ndarray: Array of simulated observations.
    """
    B = np.random.binomial(1, lambda_, size=m)
    Z = np.random.standard_normal(size=m)
    X = np.where(B == 1, mu1 + sigma1 * Z, mu2 + sigma2 * Z)
    return X

# -----------------------
# Example usage:
# (Make sure that LossReturns is defined as your 2000x2 NumPy array.)
portfolio_returns = 0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]

# Run the EM algorithm (which uses LossReturns for initialization).
parameters = EM_algorithm(portfolio_returns, LossReturns)
print("Estimated Parameters (EM):", parameters)
# The returned tuple is (lambda, mu1, sigma1, mu2, sigma2).

# -----------------------
# Monte Carlo Simulation for VaR Estimation:
num_sim = 10
sample_size = 100000
Tot = 0.0

lamda, mu1, sigma1, mu2, sigma2 = parameters

for t in range(num_sim):
    MC = simulate_mixture_observations(sample_size, lamda, mu1, mu2, sigma1, sigma2)
    Tot += np.percentile(MC, 99)

VaR_99_monte_carlo = Tot / num_sim
print("Estimated 99% VaR from Monte Carlo simulation:", VaR_99_monte_carlo)

import numpy as np
from scipy.stats import norm

def EM_algorithm(portfolio_returns, LossReturns, tol=1e-6):
    """
    Performs the EM algorithm exactly as in the R code.
    """
    n = len(portfolio_returns)

    # Initialize parameters (matching R exactly)
    lamda = 0.3
    mu1 = np.mean(LossReturns[:, 0])
    sigma1 = np.std(LossReturns[:, 0], ddof=1)
    mu2 = np.mean(LossReturns[:, 1])
    sigma2 = np.std(LossReturns[:, 1], ddof=1)

    # Initialize log-likelihood tracking
    L = [0]  # Store log-likelihood values
    X = 1
    j = 0

    # EM iterations
    while abs(X) > tol:
        j += 1

        # E-step: Compute responsibilities
        p1 = norm.pdf(portfolio_returns, mu1, sigma1) * lamda
        p2 = norm.pdf(portfolio_returns, mu2, sigma2) * (1 - lamda)
        total_prob = p1 + p2
        p1 /= total_prob  # Normalize to get probabilities
        p2 = 1 - p1

        # Compute log-likelihood
        Q = np.sum(p1 * (np.log(lamda) + np.log(norm.pdf(portfolio_returns, mu1, sigma1))) +
                   p2 * (np.log(1 - lamda) + np.log(norm.pdf(portfolio_returns, mu2, sigma2))))
        L.append(Q)

        # M-step: Update parameters
        lamda = np.mean(p1)
        mu1 = np.sum(p1 * portfolio_returns) / np.sum(p1)
        sigma1 = np.sqrt(np.sum(p1 * (portfolio_returns - mu1)**2) / np.sum(p1))
        mu2 = np.sum(p2 * portfolio_returns) / np.sum(p2)
        sigma2 = np.sqrt(np.sum(p2 * (portfolio_returns - mu2)**2) / np.sum(p2))

        # Update X to check for convergence
        X = L[j] - L[j - 1]

    return lamda, mu1, sigma1, mu2, sigma2

def simulate_mixture_observations(m, lambda_, mu1, mu2, sigma1, sigma2):
    """
    Simulates m observations using the exact R-style method.
    """
    bern = np.random.binomial(1, lambda_, size=m)
    MC = np.zeros(m)
    for i in range(m):
        if bern[i] == 1:
            MC[i] = mu1 + sigma1 * np.random.normal(0, 1)
        else:
            MC[i] = mu2 + sigma2 * np.random.normal(0, 1)
    return MC

# Example usage
portfolio_returns = 0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]
parameters = EM_algorithm(portfolio_returns, LossReturns)
print("Estimated Parameters (EM):", parameters)

# Monte Carlo Simulation for VaR Estimation
num_sim = 100
sample_size = 100000
Tot = 0.0

lamda, mu1, sigma1, mu2, sigma2 = parameters
for t in range(num_sim):
    MC = simulate_mixture_observations(sample_size, lamda, mu1, mu2, sigma1, sigma2)
    Tot += np.percentile(MC, 99)

VaR_99_monte_carlo = Tot / num_sim
print("Estimated 99% VaR from Monte Carlo simulation:", VaR_99_monte_carlo)

"""METHOD 4: EVT APPROACH"""

Mixture = -0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]

# Sort Mixture in ascending order
SortedMixture = np.sort(Mixture)

# Compute Hill Estimates
HillEst = np.zeros(300)

for k in range(1, 301):  # Loop from 1 to 300
    H = 0
    for i in range(1, k + 1):  # Loop from 1 to k

        H += np.log(SortedMixture[NumObs - i] / SortedMixture[NumObs - k])
    if H > 0:
        #print(H)
        HillEst[k - 1] = (H / k) ** -1  # Allow division by zero to behave like R

# Tail index estimation
kF = 100
tailIndex = HillEst[kF - 1]  # Adjust index since Python is zero-based

# Value at Risk (VaR) estimation
VaRQ4 = SortedMixture[NumObs - kF] * (kF / (NumObs * (1 - 0.99))) ** (1 / tailIndex)

print("VaR Q4:", VaRQ4)

import numpy as np

# Simulating example StockLoss data (replace with actual data)
np.random.seed(42)  # For reproducibility

# Compute Mixture
Mixture = -0.4 * LossReturns[:, 0] + 0.6 * LossReturns[:, 1]

# Sort Mixture in ascending order
SortedMixture = np.sort(Mixture)

# Compute Hill Estimates
HillEst = []
for k in range(1, 301):
    H = sum(np.log(SortedMixture[-(i+1)] / SortedMixture[-k]) for i in range(k))
    #print(H)
    HillEst.append((H / k) ** -1)

# Tail index estimation
kF = 100
tailIndex = HillEst[kF - 1]  # Adjust index since Python is zero-based

# Value at Risk (VaR) estimation
VaRQ4 = SortedMixture[-kF] * (kF / (NumObs * (1 - 0.99))) ** (1 / tailIndex)

print("VaR Q4:", VaRQ4)

"""METHOD 5: GARCH"""

# Specify model
model = arch_model(portfolio_returns, vol='GARCH', p=1, q=1, mean='Constant')

results = model.fit(disp='off')  # Suppress output during fitting

# Print the results (optional)
print(results.summary())

# Extract estimated parameters
mu = results.params['mu']
omega = results.params['omega']
alpha = results.params['alpha[1]']
beta = results.params['beta[1]']

print(f"Estimated Parameters:\nMu: {mu} \nOmega: {omega}\nAlpha: {alpha}\nBeta: {beta}")

sigma_unc = omega / (1 - alpha - beta)  # Calculate unconditional volatility

#initialize sigma series
sigma = np.zeros_like(portfolio_returns)
sigma[0] = sigma_unc

for i in range(1, len(portfolio_returns)):
    sigma[i] = omega + alpha * portfolio_returns[i-1]**2 + beta * sigma[i-1]

print(sigma)

#standardizing returns
Z = portfolio_returns / np.sqrt(sigma)

confidence_level = 0.99
# Calculate VaR
VaR_99_garch = np.percentile(Z[1:], (1 - confidence_level) * 100) * np.sqrt(sigma[-1]) + mu

print(-VaR_99_garch)
#I think this is needs to be multiplied with -1 since we are looking at losses.

"""METHOD 6: FITTING TO CLAYTON COPULA"""

#  GETTING THE VARIABLES IN ORDER TO CALCULATE KENDALLS TAU

n = len(ReturnLoss)
rank1 = pd.Series(stock1).rank()
rank2 = pd.Series(stock2).rank()

concordant_pairs = 0
discordant_pairs = 0

for i in range(n-1):
    for j in range(i + 1, n):
        if (rank1[i] < rank1[j] and rank2[i] < rank2[j]) or (rank1[i] > rank1[j] and rank2[i] > rank2[j]):
            concordant_pairs += 1
        elif (rank1[i] < rank1[j] and rank2[i] > rank2[j]) or (rank1[i] > rank1[j] and rank2[i] < rank2[j]):
            discordant_pairs += 1

kendalls_tau = (concordant_pairs - discordant_pairs) / (0.5 * n * (n - 1))

print("Kendalls Tau =:", kendalls_tau)

theta_est = 2 / (1-kendalls_tau) - 2
print("this is theta_est", theta_est)


lambdaU = 2**(-1/theta_est)

#MAKING THE EMPIRICAL LAMBDA
lambda_empirical = np.zeros(K)

for k in range(K):
    for i in range(n):
        if stock1[i]> np.quantile(stock1,(n -k)/ n) and stock2[i] > np.quantile(stock2,(n-k)/ n):
            lambda_empirical[k] = lambda_empirical[k] + 1
    lambda_empirical[k] = lambda_empirical[k] / k

#plotting the empirical lambda
#plt.plot(lambda_empirical)

import numpy as np
import pandas as pd
from scipy.stats import kendalltau, t, rankdata
from copulas.bivariate import ClaytonCopula
from scipy.stats import t as student_t
import matplotlib.pyplot as plt

# Assuming StockLoss is a NumPy array of shape (2000, 2)
n = 2000
LossReturns = np.random.randn(n, 2)  # Replace with actual data

# Ranking the losses
Ranking1 = rankdata(LossReturns[:, 0])
Ranking2 = rankdata(LossReturns[:, 1])
Ranked = np.column_stack((Ranking1, Ranking2)) / (n + 1)
Reverse = 1 - Ranked

# Compute Kendall's tau
_tau_hat, _ = kendalltau(Ranking1, Ranking2)

# Compute theta_hat for Clayton copula
theta_hat = 2 / (1 - _tau_hat) - 2

# Lambda Upper tail
LambdaU = 2 ** (-1 / theta_hat)

# Empirical lambda calculation
lambdaEmp = np.zeros(501)

for k in range(501):
    count = 0
    threshold_1 = np.quantile(LossReturns[:, 0], (n - k) / n)
    threshold_2 = np.quantile(LossReturns[:, 1], (n - k) / n)

    for i in range(n):
        if LossReturns[i, 0] > threshold_1 and LossReturns[i, 1] > threshold_2:
            count += 1

    if k > 0:
        lambdaEmp[k] = count / k

# Plot lambdaEmp
tl = np.arange(501)
plt.plot(tl, lambdaEmp)
plt.title("Lambda Empirical")
plt.show()

# Fitting t-distributions
fit1_params = student_t.fit(StockLoss[:, 0])
fit2_params = student_t.fit(StockLoss[:, 1])

mu1, sigma1, df1 = fit1_params[1], fit1_params[2], fit1_params[0]
mu2, sigma2, df2 = fit2_params[1], fit2_params[2], fit2_params[0]

# Clayton copula
clayton_copula = ClaytonCopula(theta=theta_hat)

# Simulating from copula
Q6 = 0
for _ in range(300):
    copula_samples = clayton_copula.sample(100000)
    x_copula = student_t.ppf(1 - copula_samples[:, 0], df1) * sigma1 + mu1
    y_copula = student_t.ppf(1 - copula_samples[:, 1], df2) * sigma2 + mu2
    Q6 += np.quantile(0.4 * x_copula + 0.6 * y_copula, 0.99)

VaRQ6 = Q6 / 300
print("VaR Q6:", VaRQ6)

#fitting distributions
fitting1 = t.fit(stock1)
fitting2 = t.fit(stock2)

#getting the parameters
mu1 = fitting1[1]
mu2 = fitting2[1]
sigma1 = fitting1[2]
sigma2 = fitting2[2]
df1 = fitting1[0]
df2 = fitting2[0]

print(fitting1)
print(mu1, mu2, sigma1, sigma2, df1, df2)

kendalls_tau = (concordant_pairs - discordant_pairs) / (0.5 * n * (n - 1))
theta_est = 2 / (1-kendalls_tau) - 2

#simulating using the clayton copula
num_simulations = 1000

clayton = Clayton(theta_est)
clayton.tau = kendalls_tau
clayton.theta = theta_est

copula_sample = clayton.sample(num_simulations)

VaR_99_copula = 0

for i in range(num_simulations):


    x_copula = t.ppf(copula_sample[:, 0], df1, loc=mu1, scale=sigma1)
    y_copula = t.ppf(copula_sample[:, 1], df2, loc=mu2, scale=sigma2)

    portfolio_returns_copula = weights[0] * x_copula + weights[1] * y_copula

    VaR_99_copula += np.quantile(portfolio_returns_copula, 0.99)

# Calculate average VaR
VaR_99_copula = VaR_99_copula / num_simulations


print(VaR_99_copula)

