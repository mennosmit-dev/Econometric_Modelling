**For full details feel free to download the pdf in the map.**

# 📊 Income Level Prediction with Random Forests 🌳

## 🔍 Overview

This project implements a machine learning pipeline to predict income levels based on tabular data using Random Forest classifiers. It performs feature selection, hyperparameter tuning, model training, and final prediction generation with nested cross-validation to ensure robust evaluation.

The code was originally developed in a Jupyter notebook and later cleaned up for clarity and reproducibility.

---

## 📁 Contents

- `Predicting_Income_Level.ipynb` / `.py`  
  Main script that:  
  - Loads data and a codebook describing features.  
  - Performs nested cross-validation for hyperparameter tuning and feature selection.  
  - Saves tuning results and visualizes the impact of hyperparameters.  
  - Trains a final Random Forest model on selected features.  
  - Generates predictions on the test dataset and saves them to a text file.

- `codebook.csv` 📖  
  CSV file describing features and their meanings.

- `train.csv` 🏋️‍♂️  
  Training dataset with features and target variable.

- `test.csv` 🧪  
  Test dataset with features only.

- `oos_accuracies.csv` 📊  
  Output file containing cross-validation results for hyperparameter tuning and feature selection.

- `mean_accuracy_hyperparameters.png` 📈  
  Visualization of mean accuracy by number of features and hyperparameter combinations.

- `features_prediction.png` 🌟  
  Bar plot of feature importances used in the final model.

- `predictions.txt` 📝  
  Final predictions on the test set generated by the model.

---

## ⚙️ Requirements

- Python 3.7 or later  
- Packages:  
  - numpy ➕  
  - pandas 🐼  
  - matplotlib 🎨  
  - scikit-learn 🤖  
  - tqdm ⏳  

Install the packages using pip if needed:

HAI!bash
pip install numpy pandas matplotlib scikit-learn tqdm
HAI!

---

## 🚀 How to Run

1. Place the dataset files (`train.csv`, `test.csv`, and `codebook.csv`) in the working directory.

2. Run the main script (`Predicting_Income_Level.ipynb` or `.py`) to:

   - Load and prepare data.  
   - Perform nested cross-validation to tune hyperparameters and select important features.  
   - Save tuning results and generate visualizations.  
   - Train the final model and produce predictions for the test dataset.  
   - Save predictions to `predictions.txt`.

3. View output plots (`mean_accuracy_hyperparameters.png`, `features_prediction.png`) for insights on hyperparameter tuning and feature importance.

---

## 📂 Output Files

- `oos_accuracies.csv`: Cross-validation results during tuning.  
- `mean_accuracy_hyperparameters.png`: Accuracy visualization for different hyperparameter combinations and feature counts.  
- `features_prediction.png`: Feature importance bar plot.  
- `predictions.txt`: Final predictions on test data.

---

## 💡 Notes

- Random Forest models do not require feature standardization or normalization.  
- Feature selection is based on feature importance with a threshold to reduce dimensionality.  
- Nested cross-validation ensures unbiased performance estimation and hyperparameter selection.  
- Hyperparameters tuned include number of trees, max depth, min samples per split/leaf, and max features.

---

Feel free to reach out if you have questions or want to contribute! 🙌
